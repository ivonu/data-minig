\documentclass[a4paper,11pt,twocolumn]{article}

\author{Matthias Ganz}

% Two A4-pages (i.e. one A4-sheet of paper), either handwritten or 11 point minimum font size

\usepackage{enumitem}
\usepackage{url}
\usepackage{graphicx}	% For figure environment
\usepackage[center]{subfigure}
\usepackage{amssymb}	% For mathematical figures like \mathbb{R}
\usepackage{amsmath}
\usepackage{framed}
\usepackage{epstopdf}
\usepackage{geometry}
\geometry{a4paper, top=6mm, left=7mm, right=7mm, bottom=8mm, headsep=0mm, footskip=0mm}


\usepackage[compact]{titlesec}

\begin{document}


\section{Approximate Retrieval}
Dataset $S \subset \mathbb{R}^D$ , distance function $d: \mathbb{R}^D \times \mathbb{R}^D \leftarrow \mathbb{R}$

Nearest neighbour $s^* = \arg\min \limits_{s \in S} d(q,s) $ for query $q$

Near duplicate detection: find all s,s' in S with distance at most $\epsilon$

\subsection{Distance Function $d: S \times S  \rightarrow \mathbb{R}$}

$\forall s,t \in S : d(s,t) \geq 0$

$\forall s: d(s,s) = 0$

$\forall s,t:d(s,t) = d(t,s)$

$\forall s,t,r : d(s,t) + d(t,r) \geq d(s,r)$

Cosine Distance: $d(x,y) = arccos  {{x^T y}\over{ ||x||_2 ||y||_2}} $

Jaccard Similarity: $Sim(A,B)$

Jaccard Dist.: $d(A,B) = 1 - {{A \cap B}\over{A \cup B}} = 1 - Sim(A,B) $

\subsection{k-Shingle}
Represent documents as a set of k-Shingles.

\subsection{Min-Hashing for Jaccard Distance}
Apply row permutation $\pi$ to shingle matrix (shingle/documents). Min-Hash for column $C: h(C) =$ minimum row index which contains a 1.

Holds: $P[h(C1) = h(C2)] = Sim (C1,C2)$

Use multiple permutations. Signature matrix $M$ (permutation/document) is now split into bands. Each band vector (Min-Hash values from permutation n to n+b of one document) is now hashed to another bucket. If a document is hashed to the same bucket for at least one band as another then both are similar.


\subsubsection{$(d_1,d_2,p_1,p_2)$-sensitive Hash-Functions}

$\forall x,y \in S : d(x,y) \leq  d_1 \Rightarrow P[h(x)=h(y)] \geq p_1$

$\forall x,y \in S : d(x,y) \geq  d_2 \Rightarrow P[h(x)=h(y)] \leq p_2$

\subsubsection{r-way AND}
Convert hash family $F$ to $F'$. Each member of $F'$ consists of a vector of r hash functions from $F$

$F'$ is $(d_1, d_2, p_1^r, p_2^r)$-sensitive, decreases false positives.

\subsubsection{b-way OR}
Convert hash family $F$ to $F'$. Each member of $F'$ consists of a vector of r hash functions from $F$

$F'$ is $(d_1, d_2, 1-(1-p_1)^b, 1-(1-p_2)^b)$-sensitive, decreases false negatives.

AND-OR: $(d_1,d_2,1-(1-p_1^r)^b,1-(1-p_2^r)^b)$

OR-AND: $(d_1,d_2,1-((1-p_1)^b)^r,1-((1-p_2)^b)^r)$




\subsection{LSH for euclidean Distance}
Map points to random line with "buckets". Pick $w \in \mathbb{R}, ||w||_2 = 1$ and $b \in [0,a]$ unif. with bucket-width $a$.

$h_{w,b}(x) = 
\lfloor
{{w^Tx + b}\over{a}}
\rfloor$

\subsection{LSH for cosine Distance}
Pick $w \in \mathbb{R}^d$ with $||w||_2=1$ uniform at random.

$h_w(x) = sign(w^Tx)$

\section{Classification}

\subsection{Online SVM}
Classify new datapoint $x_t$ with $y_t = sign(w_t^Tx_t)$, incur loss $l_t = \max (0,1-y_t w_t^T x_t)$ Update $w_{t+1} = w_t - \eta_t \nabla f_t(w_t)$

Project back: $Proj_X(w) = arg \min \limits_{w' \in S} ||w' - w||_2$

Regret $R_T = (\sum \limits_{t=1}^T l_t) - \min \limits_{w \in S} \sum \limits_{t=1}^T f_t(w)$

For OCP Update: if $y_t w_t^T x_t < 1$ then 
$w'=w_t+\eta_t y_t x_t$
$w_t+1 = w' \min (1,{{1}\over{\sqrt{\lambda w'^T w'}}})$


\section{Active Learning}
Online learning (stream based) active learning: Data arrives, we decide whether to request label or not.

Pool-based learning: Unlearned dataset, can request labels for points.

\subsection{Uncertainty Sampling}
For SVM chose points close to decision boundary. Use LSH for cosine distance on line orthogonal to boundary $w$.

\subsection{Informative Sampling}
Sample to get maximal reduction of version space.


\section{Online k-Means}
\begin{itemize}[noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt]
\item for $t=1:N$
\begin{itemize}[noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt]
\item find $c = arg \min \limits_j ||\mu_j - x_t ||_2$
\item set $ \mu_c = \mu_c+ \eta_t ( x_t - \mu_c)$
\end{itemize}
\end{itemize}

\subsection{Coresets}



\section{Linear Algebra}

\subsection{Convex Functions}
$\forall x,y, \lambda \in [0,1] : \lambda f(x) + (1-\lambda)f(y) \geq f(\lambda x +  (1-\lambda) y)$

\subsection{H-Strong Convex}
$f(y) \geq f(x) + \nabla f(x)^T(y-x) + {H \over 2 } || y - x ||_2^2$

\subsection{Subgradient Functions}
$\forall x' \in S : f(x') \geq f(x) + g_x^T(x'-x)$

\subsection{Vector Norms}
are positive scalable, full-fill the triangular inequality, norm of 0 is 0

\subsubsection{p-Norm}
$ || x ||_p = \left( \sum_{i=1}^{n}{|x_i|^p} \right)^{\frac{1}{p}}$

\subsubsection{Euclidean Norm} 
p-Norm where $p=2$

\subsubsection{1-Norm}
Manhattan-Norm
$ ||x||_1 = \sum_{i=1}^{n}{|x_i|} $

\subsubsection{Zero-Norm} 
counts the number of non-zero entries.

\subsection{Matrix Norms}

\subsubsection{Nuclear Norm}

$|| . ||_*$ sum of singular values

\subsubsection{Frobenious-Norm}
$sqrt(sum(sum(A.^2)))$

\subsubsection{Spectral Norm} 
Largest singular value if square 

$||A||_2 = \sigma_{max}(A)$ 
~~Is equals to the 2-Norm

\subsubsection{Induced Matrix Norms}
$ ||A|| = max \left( \frac{ ||Ax|| }{ ||x|| } \right)$

\subsection{Orthogonality}

\subsubsection{Vectors} 

inner (scalar) product $\langle ~.~,~.~ \rangle = 0$

\subsubsection{Matrices} 

quadratic, values are in $\mathbb{R}$, $Q^T = Q^{-1}$

\subsubsection{Functions}

$f(x)$ orth. to $g(x)$ if $0 = \int f(x) g(x) dx $

\subsubsection{Coherence}

$m(U)= max_{i,j:i\neq j} | u_i^T u_j|$

\subsubsection{Convexity}

$f(\theta x + (1-\theta)y) \leq \theta f(x) + (1-\theta) f(y)$


\section{Convex optimisation}

minimize $f(x)$ subject to 

$g_i(x) \leq 0, i = 1,...,m$ inequality constr.

$h_i(x) = 0, i = 1,...,p$ equality constr.

Create the Lagrangian

$L(x, \lambda, \nu) = f(x) + \sum_{i=1}^m{\lambda_i g_i(x) + \sum_{i=1}^p{\nu_i h_i(x)}}$

Lagrange dual function: $ d(\lambda, \nu) = \inf_{x} L (x,\lambda, \nu) $

Lagrange dual problem: max. $d(\lambda, \nu)$  subj. to $\lambda \geq 0$

\section{Sidenotes}

\textbf{Curse of Dimensionality}: The more dimensions the closer data points get to each other on average in terms of euclidean distance.

\end{document}